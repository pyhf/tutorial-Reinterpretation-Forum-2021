{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood Serialization and Patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using HEPData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preserved on HEPData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of this tutorial, ATLAS has published 4 full likelihoods to HEPData\n",
    "\n",
    "<p align=\"center\">\n",
    "<a href=\"https://www.hepdata.net/record/ins1755298?version=3\"><img src=\"https://raw.githubusercontent.com/matthewfeickert/talk-SciPy-2020/e0c509cd0dfef98f5876071edd4c60aff9199a1b/figures/HEPData_likelihoods.png\"></a>\n",
    "</p>\n",
    "\n",
    "Let's explore the 1Lbb workspace a little bit shall we?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Data\n",
    "\n",
    "We'll use python `requests` and `tarfile` to download the tarball and extract the files we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import hashlib\n",
    "import tarfile\n",
    "import json\n",
    "\n",
    "electroweakino_HEPData_URL = \"https://www.hepdata.net/record/resource/1408476?view=true\"\n",
    "targz_filename = \"data/1Lbb_workspaces.tar.gz\"\n",
    "response = requests.get(electroweakino_HEPData_URL, stream=True)\n",
    "assert response.status_code == 200\n",
    "with open(targz_filename, \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "hashlib.sha256(open(targz_filename, \"rb\").read()).hexdigest()\n",
    "# Open as a tarfile\n",
    "tar_archive = tarfile.open(targz_filename, \"r:gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate our objects\n",
    "\n",
    "To make this easier on us, we'll first define a nice helper function to extract out a file from the tarball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_from_tarfile(tarfile, json_name):\n",
    "    json_file = tarfile.extractfile(tarfile.getmember(json_name)).read().decode(\"utf8\")\n",
    "    return json.loads(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and `import pyhf` and get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhf\n",
    "\n",
    "spec = get_json_from_tarfile(tar_archive, \"BkgOnly.json\")\n",
    "patchset = pyhf.PatchSet(get_json_from_tarfile(tar_archive, \"patchset.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what did the analyzers give us for signal patches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patching in Signals\n",
    "\n",
    "Let's look at this [`pyhf.PatchSet`](https://scikit-hep.org/pyhf/_generated/pyhf.patchset.PatchSet.html#pyhf.patchset.PatchSet) object which provides a user-friendly way to interact with many signal patches at once.\n",
    "\n",
    "### PatchSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyhf.patchset.PatchSet object with 125 patches at 0x7fb112485350>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patchset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh wow, we've got 125 patches. What information does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'description: {patchset.description}')\n",
    "print(f'    digests: {patchset.digests}')\n",
    "print(f'     labels: {patchset.labels}')\n",
    "print(f' references: {patchset.references}')\n",
    "print(f'    version: {patchset.version}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've got a useful description of the signal patches... there's a digest. Does that match the background-only workspace we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2563672e1a165384faf49f1431e921d88c9c07ec10f150d5045576564f98f820'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyhf.utils.digest(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does! In fact, this sort of verification check will be done automatically when applying patches using `pyhf.PatchSet` as we will see shortly. To manually verify, simply run `pyhf.PatchSet.verify` on the workspace. No error means everything is fine. It will loudly complain otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchset.verify(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No error, whew. Let's move on.\n",
    "\n",
    "The labels `m1` and `m2` tells us that we have the signal patches parametrized in 2-dimensional space, likely as $m_1 = \\tilde{\\chi}_1^\\pm$ and $m_2 = \\tilde{\\chi}_1^0$... but I guess we'll see?\n",
    "\n",
    "The references list the references for this dataset, which is pointing at the hepdata record for now.\n",
    "\n",
    "Next, the version is the version of the schema set we're using with `pyhf` (`1.0.0`).\n",
    "\n",
    "And last, but certainly not least... its patches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyhf.patchset.Patch object 'C1N2_Wh_hbb_1000_0(1000, 0)' at 0x7fb17c589b90>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_1000_100(1000, 100)' at 0x7fb11c309090>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_1000_150(1000, 150)' at 0x7fb1124857d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_1000_200(1000, 200)' at 0x7fb1124858d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_1000_250(1000, 250)' at 0x7fb112485c10>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_1000_300(1000, 300)' at 0x7fb112485990>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_1000_350(1000, 350)' at 0x7fb112485710>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_1000_400(1000, 400)' at 0x7fb112485d50>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_1000_50(1000, 50)' at 0x7fb112485810>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_150_0(150, 0)' at 0x7fb112485c90>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_165_35(165, 35)' at 0x7fb112485cd0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_175_0(175, 0)' at 0x7fb11c2fbc10>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_175_25(175, 25)' at 0x7fb11c30e5d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_190_60(190, 60)' at 0x7fb11248e090>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_200_0(200, 0)' at 0x7fb11248e3d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_200_25(200, 25)' at 0x7fb11248e2d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_200_50(200, 50)' at 0x7fb11248e350>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_225_0(225, 0)' at 0x7fb11248e410>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_225_25(225, 25)' at 0x7fb11248e390>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_225_50(225, 50)' at 0x7fb11248e190>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_250_0(250, 0)' at 0x7fb11248e1d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_250_100(250, 100)' at 0x7fb11248e110>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_250_25(250, 25)' at 0x7fb11248e150>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_250_50(250, 50)' at 0x7fb11248e490>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_250_75(250, 75)' at 0x7fb11248e510>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_275_0(275, 0)' at 0x7fb11248e590>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_275_25(275, 25)' at 0x7fb11248e610>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_275_50(275, 50)' at 0x7fb11248e4d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_275_75(275, 75)' at 0x7fb11248e5d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_300_0(300, 0)' at 0x7fb11248e750>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_300_150(300, 150)' at 0x7fb11248e550>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_300_25(300, 25)' at 0x7fb11248e650>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_300_50(300, 50)' at 0x7fb11248e710>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_300_75(300, 75)' at 0x7fb11248e790>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_325_0(325, 0)' at 0x7fb11248e6d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_325_50(325, 50)' at 0x7fb11248e690>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_350_0(350, 0)' at 0x7fb11248e7d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_350_100(350, 100)' at 0x7fb11248e810>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_350_150(350, 150)' at 0x7fb11248e850>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_350_200(350, 200)' at 0x7fb11248e890>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_350_25(350, 25)' at 0x7fb11248e8d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_350_50(350, 50)' at 0x7fb11248e910>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_350_75(350, 75)' at 0x7fb11248e950>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_375_0(375, 0)' at 0x7fb11248e990>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_375_50(375, 50)' at 0x7fb11248e9d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_400_0(400, 0)' at 0x7fb11248ea10>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_400_100(400, 100)' at 0x7fb11248ea50>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_400_150(400, 150)' at 0x7fb11248ea90>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_400_200(400, 200)' at 0x7fb11248ead0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_400_25(400, 25)' at 0x7fb11248eb10>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_400_250(400, 250)' at 0x7fb11248eb50>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_400_50(400, 50)' at 0x7fb11248eb90>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_425_0(425, 0)' at 0x7fb11248ebd0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_450_0(450, 0)' at 0x7fb11248ec10>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_450_100(450, 100)' at 0x7fb11248ec50>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_450_150(450, 150)' at 0x7fb11248ec90>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_450_200(450, 200)' at 0x7fb11248ecd0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_450_250(450, 250)' at 0x7fb11248ed10>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_450_300(450, 300)' at 0x7fb11248ed50>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_450_50(450, 50)' at 0x7fb11248ed90>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_500_0(500, 0)' at 0x7fb11248edd0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_500_100(500, 100)' at 0x7fb11248ee10>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_500_150(500, 150)' at 0x7fb11248ee50>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_500_200(500, 200)' at 0x7fb11248ee90>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_500_250(500, 250)' at 0x7fb11248eed0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_500_300(500, 300)' at 0x7fb11248ef10>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_500_350(500, 350)' at 0x7fb11248ef50>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_500_50(500, 50)' at 0x7fb11248ef90>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_535_400(535, 400)' at 0x7fb11248efd0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_550_0(550, 0)' at 0x7fb11249c050>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_550_100(550, 100)' at 0x7fb11249c090>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_550_150(550, 150)' at 0x7fb11249c0d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_550_200(550, 200)' at 0x7fb11249c110>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_550_250(550, 250)' at 0x7fb11249c150>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_550_300(550, 300)' at 0x7fb11249c190>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_550_50(550, 50)' at 0x7fb11249c1d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_600_0(600, 0)' at 0x7fb11249c210>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_600_100(600, 100)' at 0x7fb11249c250>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_600_150(600, 150)' at 0x7fb11249c290>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_600_200(600, 200)' at 0x7fb11249c2d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_600_250(600, 250)' at 0x7fb11249c310>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_600_300(600, 300)' at 0x7fb11249c350>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_600_350(600, 350)' at 0x7fb11249c390>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_600_400(600, 400)' at 0x7fb11249c3d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_600_50(600, 50)' at 0x7fb11249c410>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_650_0(650, 0)' at 0x7fb11249c450>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_650_100(650, 100)' at 0x7fb11249c490>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_650_150(650, 150)' at 0x7fb11249c4d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_650_200(650, 200)' at 0x7fb11249c510>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_650_250(650, 250)' at 0x7fb11249c550>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_650_300(650, 300)' at 0x7fb11249c590>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_650_50(650, 50)' at 0x7fb11249c5d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_700_0(700, 0)' at 0x7fb11249c610>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_700_100(700, 100)' at 0x7fb11249c650>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_700_150(700, 150)' at 0x7fb11249c690>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_700_200(700, 200)' at 0x7fb11249c6d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_700_250(700, 250)' at 0x7fb11249c710>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_700_300(700, 300)' at 0x7fb11249c750>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_700_350(700, 350)' at 0x7fb11249c790>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_700_400(700, 400)' at 0x7fb11249c7d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_700_50(700, 50)' at 0x7fb11249c810>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_750_100(750, 100)' at 0x7fb11249c850>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_750_150(750, 150)' at 0x7fb11249c890>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_750_200(750, 200)' at 0x7fb11249c8d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_750_250(750, 250)' at 0x7fb11249c910>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_750_300(750, 300)' at 0x7fb11249c950>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_750_50(750, 50)' at 0x7fb11249c990>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_800_0(800, 0)' at 0x7fb11249c9d0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_800_100(800, 100)' at 0x7fb11249ca10>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_800_150(800, 150)' at 0x7fb11249ca50>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_800_200(800, 200)' at 0x7fb11249ca90>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_800_250(800, 250)' at 0x7fb11249cad0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_800_300(800, 300)' at 0x7fb11249cb10>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_800_350(800, 350)' at 0x7fb11249cb50>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_800_400(800, 400)' at 0x7fb11249cb90>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_800_50(800, 50)' at 0x7fb11249cbd0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_900_0(900, 0)' at 0x7fb11249cc10>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_900_100(900, 100)' at 0x7fb11249cc50>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_900_150(900, 150)' at 0x7fb11249cc90>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_900_200(900, 200)' at 0x7fb11249ccd0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_900_250(900, 250)' at 0x7fb11249cd10>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_900_300(900, 300)' at 0x7fb11249cd50>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_900_350(900, 350)' at 0x7fb11249cd90>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_900_400(900, 400)' at 0x7fb11249cdd0>,\n",
       " <pyhf.patchset.Patch object 'C1N2_Wh_hbb_900_50(900, 50)' at 0x7fb11249ce10>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patchset.patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see all the patches listed both by name such as `C1N2_Wh_hbb_900_250` as well as a pair of points `(900, 250)`. Why is this useful? The `PatchSet` object acts like a special dictionary look-up where it will grab the patch you need based on the unique key you provide it.\n",
    "\n",
    "For example, we can look up by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyhf.patchset.Patch object 'C1N2_Wh_hbb_900_250(900, 250)' at 0x7fb11249cd10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patchset['C1N2_Wh_hbb_900_250']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or by the pair of points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyhf.patchset.Patch object 'C1N2_Wh_hbb_900_250(900, 250)' at 0x7fb11249cd10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patchset[(900, 250)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patches\n",
    "\n",
    "A `pyhf.PatchSet` is a collection of `pyhf.Patch` objects. What is a patch indeed? It contains enough information about how to apply the signal patch to the corresponding background-only workspace (matched by digest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name: C1N2_Wh_hbb_900_250\n",
      "values: (900, 250)\n"
     ]
    }
   ],
   "source": [
    "patch = patchset['C1N2_Wh_hbb_900_250']\n",
    "print(f'  name: {patch.name}')\n",
    "print(f'values: {patch.values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most importantly, it contains the patch information itself. Specifically, this inherits from the `jsonpatch.JsonPatch` object, which is a 3rd party module providing native support for json patching in python. That means we can simply apply the patch to our workspace directly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " samples pre-patch: ['diboson', 'multiboson', 'singletop', 'ttbar', 'tth', 'ttv', 'vh', 'wjets', 'zjets']\n",
      "samples post-patch: ['C1N2_Wh_hbb_900_250', 'diboson', 'multiboson', 'singletop', 'ttbar', 'tth', 'ttv', 'vh', 'wjets', 'zjets']\n"
     ]
    }
   ],
   "source": [
    "print(f' samples pre-patch: {pyhf.Workspace(spec).samples}')\n",
    "print(f'samples post-patch: {pyhf.Workspace(patch.apply(spec)).samples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, more quickly, from the `PatchSet` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " samples pre-patch: ['diboson', 'multiboson', 'singletop', 'ttbar', 'tth', 'ttv', 'vh', 'wjets', 'zjets']\n",
      "samples post-patch: ['C1N2_Wh_hbb_900_250', 'diboson', 'multiboson', 'singletop', 'ttbar', 'tth', 'ttv', 'vh', 'wjets', 'zjets']\n"
     ]
    }
   ],
   "source": [
    "print(f' samples pre-patch: {pyhf.Workspace(spec).samples}')\n",
    "print(f'samples post-patch: {pyhf.Workspace(patchset.apply(spec, (900, 250))).samples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patching via Model Creation\n",
    "\n",
    "One last way to apply the patching is to, instead of patching workspaces, we patch the models as we build them from the background-only workspace. This maybe makes it easier to treat the background-only workspace as immutable, and patch in signal models when grabbing the model. Check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = pyhf.Workspace(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load up our background-only spec into the workspace. Then let's create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples (workspace): ['diboson', 'multiboson', 'singletop', 'ttbar', 'tth', 'ttv', 'vh', 'wjets', 'zjets']\n",
      "samples (  model  ): ['C1N2_Wh_hbb_900_250', 'diboson', 'multiboson', 'singletop', 'ttbar', 'tth', 'ttv', 'vh', 'wjets', 'zjets']\n"
     ]
    }
   ],
   "source": [
    "model = workspace.model(patches=[patchset['C1N2_Wh_hbb_900_250']])\n",
    "print(f'samples (workspace): {workspace.samples}')\n",
    "print(f'samples (  model  ): {model.config.samples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing Physics\n",
    "\n",
    "So we want to try and reproduce part of the contour. At least convince ourselves we're doing *physics* and not *fauxsics*. ... Anyway... Let's remind ourselves of the 1Lbb contour as we don't have the photographic memory of the ATLAS SUSY conveners\n",
    "\n",
    "<img alt=\"1Lbb exclusion contour\" src=\"https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SUSY-2019-08/fig_06.png\" width=\"600\" />\n",
    "\n",
    "So let's work around the 700-900 GeV $\\tilde{\\chi}_1^\\pm, \\tilde{\\chi}_2^0$ region. We'll look at two points here:\n",
    "\n",
    "* `C1N2_Wh_hbb_650_0(650, 0)` which is below the contour and excluded\n",
    "* `C1N2_Wh_hbb_1000_0(1000, 0)` which is above the contour and not excluded\n",
    "\n",
    "Let's perform a \"standard\" hypothesis test (with $\\mu = 1$ null BSM hypothesis) on both of these and use the $\\text{CL}_\\text{s}$ values to convince ourselves that we just did reproducible physics!?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolators?\n",
    "\n",
    "Ok, one last thing I should mention for the HistFitter users. We use nonlinear interpolators by default for the `normsys` and `histosys` modifiers. These correspond to `code4` and `code4p` in the [HistFactory paper](https://cds.cern.ch/record/1456844). We'll additionally configure the modifier settings to switch to these interpolators by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifier_settings={\n",
    "    \"normsys\": {\"interpcode\": \"code4\"},\n",
    "    \"histosys\": {\"interpcode\": \"code4p\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing Physics, for real now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_below = workspace.model(\n",
    "    patches=[patchset['C1N2_Wh_hbb_650_0']],\n",
    "    modifier_settings=modifier_settings\n",
    ")\n",
    "\n",
    "model_above = workspace.model(\n",
    "    patches=[patchset['C1N2_Wh_hbb_1000_0']],\n",
    "    modifier_settings=modifier_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've made our models. Let's test hypotheses!\n",
    "\n",
    "*Note: this will not be as instantaneous as our simple models.. but it should still be pretty fast!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/feickert/.venvs/louie-test/lib/python3.7/site-packages/pyhf/tensor/numpy_backend.py:271: RuntimeWarning: invalid value encountered in log\n",
      "  return n * np.log(lam) - lam - gammaln(n + 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed: 0.01713749945809046, Expected band: [4.8418614774961446e-06, 9.133176119925303e-05, 0.0014659909896557642, 0.017328691703589985, 0.12149939119053965]\n"
     ]
    }
   ],
   "source": [
    "result_below = pyhf.infer.hypotest(\n",
    "    1.0,\n",
    "    workspace.data(model_below),\n",
    "    model_below,\n",
    "    qtilde=True,\n",
    "    return_expected_set=True\n",
    ")\n",
    "print(f\"Observed: {result_below[0]}, Expected band: {[exp.tolist() for exp in result_below[1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed: 0.5856783902261125, Expected band: [0.049880381089905654, 0.12644650624454196, 0.2925782875240953, 0.5694124401526922, 0.8475953637671176]\n"
     ]
    }
   ],
   "source": [
    "result_above = pyhf.infer.hypotest(\n",
    "    1.0,\n",
    "    workspace.data(model_above),\n",
    "    model_above,\n",
    "    qtilde=True,\n",
    "    return_expected_set=True\n",
    ")\n",
    "print(f\"Observed: {result_above[0]}, Expected band: {[exp.tolist() for exp in result_above[1]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as you can see, we're getting results that we generally expect. Excluded models are those for which $\\text{CL}_\\text{s} < 0.05$. Additionally, you can see that the expected bands $-2\\sigma$ for the $(1000, 0)$ point is just slightly below the observed result for the $(650, 0)$ point which is what we observe in the figure above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
